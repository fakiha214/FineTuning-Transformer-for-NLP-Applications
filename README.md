# FineTuning-Transformer-for-NLP-Applications
This project focuses on applying three distinct Transformer architectures (Encoder-only, Decoder-only, and Encoderâ€“Decoder) to real-time NLP tasks. You need to fine-tune pretrained models from the Hugging Face library for classification, code generation, and summarization, showcasing their comparative strengths.
